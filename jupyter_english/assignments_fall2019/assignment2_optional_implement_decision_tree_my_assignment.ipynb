{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Author: [Yury Kashnitsky](https://yorko.github.io) (@yorko). Edited by Anna Tarelina (@feuerengel). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment #2. Optional part\n",
    "## <center> Implementation of the decision tree algorithm\n",
    "    \n",
    "#  <center>  <font color = 'red'> Warning! </font>This is a very useful but ungraded assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scipy_stats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_classification, make_regression, load_digits, load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix `random_state` (a.k.a. random seed) beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Implement the class `DecisionTree`**\n",
    "**Specification:**\n",
    "- the class is inherited from `sklearn.BaseEstimator`;\n",
    "- class constructor has the following parameters: \n",
    "    `max_depth` - maximum depth of the tree (`numpy.inf` by default); \n",
    "    `min_samples_split` - the minimum number of instances in a node for a splitting to be done (2 by default); \n",
    "    `criterion` - split criterion ('gini' or 'entropy' for classification, 'variance' or 'mad_median' for regression; 'gini' by default);\n",
    "    \n",
    "    A functional to be maximized to find an optimal partition at a given node has the form\n",
    "    $$Q(X, j, t) = F(X) - \\dfrac{|X_l|}{|X|} F(X_l) - \\dfrac{|X_r|}{|X|} F(X_r),$$\n",
    "    where $X$ are samples at a given node, $X_l$ and $X_r$ are partitions of samples $X$ into two parts \n",
    "    with the following condition $[x_j < t]$, and $F(X)$ is a partition criterion.\n",
    "    \n",
    "    For classification: let $p_i$ be the fraction of the instances of the $i$-th class in the dataset $X$.\n",
    "    \n",
    "    'gini': Gini impurity $F(X) = 1 -\\sum_{i = 1}^K p_i^2$.\n",
    "    \n",
    "    'entropy': Entropy $F(X) = -\\sum_{i = 1}^K p_i \\log_2(p_i)$.\n",
    "    \n",
    "    For regression: $y_j = y(x_j)$ - is a target for an instance $x_j$, $y = (y_1, \\dots, y_{|X|})$ - is a target vector.\n",
    "    \n",
    "    'variance': Variance (mean quadratic deviation from average) $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}(y_j - \\dfrac{1}{|X|}\\sum_{x_i \\in X}y_i)^2$\n",
    "    \n",
    "    'mad_median': Mean deviation from the median $F(X) = \\dfrac{1}{|X|} \\sum_{x_j \\in X}|y_j - \\mathrm{med}(y)|$\n",
    "    \n",
    "- the class has several methods: `fit`, `predict` and `predict_proba`;\n",
    "- the`fit` method takes the matrix of instances `X` and a target vector `y` (`numpy.ndarray` objects) and returns an instance of the class `DecisionTree` representing the decision tree trained on the dataset `(X, y)` according to parameters set in the constructor; \n",
    "- the `predict_proba` method takes the matrix of instances `X` and returns the matrix `P` of a size `X.shape[0] x K`, where `K` is the number of classes and $p_{ij}$ is the probability of an instance in $i$-th row of `X` to belong to class $j \\in \\{1, \\dots, K\\}$.\n",
    "- the `predict` method takes the matrix of instances `X` and returns a prediction vector; in case of classification, prediction for an instance $x_i$ falling into leaf $L$ will be the class, mostly represented among instances in $L$. In case of regression, it'll be the mean value of targets for all instances in leaf $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __probability_distr_dict(array):\n",
    "    arr = np.array(array, dtype=int)\n",
    "    arr.flatten()\n",
    "    arr.sort()\n",
    "    countedElements = Counter(arr)\n",
    "    uniqueElements = list(dict.fromkeys(arr))\n",
    "    uniqueElements.sort()\n",
    "    prob_dict = {}\n",
    "    elements_len = len(arr)\n",
    "\n",
    "    for key in uniqueElements:\n",
    "        prob_dict[key] = countedElements[key] / elements_len\n",
    "    return prob_dict\n",
    "\n",
    "def entropy(y):\n",
    "    prob_dict = __probability_distr_dict(array=y)\n",
    "\n",
    "    result = 0\n",
    "    for key in prob_dict:\n",
    "        result += prob_dict[key] * np.log2(prob_dict[key])\n",
    "    return -1 * result if result != 0 else 0.0\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    prob_dict = __probability_distr_dict(array=y)\n",
    "\n",
    "    result = 0\n",
    "    for key in prob_dict:\n",
    "        result += prob_dict[key] ** 2\n",
    "    return 1 - result\n",
    "\n",
    "\n",
    "def variance(y):\n",
    "    arr = np.array(y, dtype=int)\n",
    "    arr.flatten()\n",
    "    a = [(x - (np.sum(arr) / len(arr))) ** 2 for x in arr]\n",
    "    result = np.sum(a) / len(a)\n",
    "    return result\n",
    "\n",
    "\n",
    "def mad_median(y):\n",
    "    arr = np.array(y, dtype=int)\n",
    "    arr.flatten()\n",
    "    a = [(x - np.median(arr)) for x in arr]\n",
    "    result = np.sum(a) / len(a)\n",
    "    return result\n",
    "\n",
    "\n",
    "def best_partition_functional_result(X=list(), y=list(), F=gini):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "    maximization_parameter = 0\n",
    "    intermediate_param = None\n",
    "    split_index = 0\n",
    "    indicies = range(0,len(X))\n",
    "    \n",
    "    for i in indicies:\n",
    "        current_left_split = y[0:i]\n",
    "        current_right_split = y[i:len(y)]\n",
    "        current_maximization_parameter = partition_functional_result(y,\n",
    "                                                                     current_left_split,\n",
    "                                                                     current_right_split,\n",
    "                                                                     F)\n",
    "        if maximization_parameter < current_maximization_parameter:\n",
    "            left_split = current_left_split\n",
    "            right_split = current_right_split\n",
    "            maximization_parameter = current_maximization_parameter\n",
    "            intermediate_param = X[i] if i == 0 else (X[i - 1] + X[i]) / 2\n",
    "            split_index = i\n",
    "    return (left_split,\n",
    "            right_split,\n",
    "            X[0:len(left_split)],\n",
    "            X[len(left_split):len(y)],\n",
    "            F(left_split),\n",
    "            F(right_split),\n",
    "            F(y),\n",
    "            intermediate_param,\n",
    "            split_index)\n",
    "\n",
    "# note! This function applies for check target parameter (which is supposed to be binary one)\n",
    "def partition_functional_result(Y, Y_l, Y_r, F=gini):\n",
    "    return F(Y) - (len(Y_l) / len(Y)) * F(Y_r) - (len(Y_r) / len(Y)) * F(Y_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Node` class implements a node in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "\n",
    "    def __init__(self, feature_idx=0, threshold=0, labels=[], left=None, right=None, criterion=Criterion.GINI):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.labels = labels\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.criterion_type = criterion\n",
    "        self.criterion_value = 0\n",
    "        self.__margin = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.left != None:\n",
    "            self.left.__margin = self.__margin + 4\n",
    "        if self.right != None:\n",
    "            self.right.__margin = self.__margin + 4\n",
    "        margin = ' ' * self.__margin\n",
    "        feature = margin + \"feature idx: \" + str(self.feature_idx)\n",
    "        str_threshhold = margin + \"threshold: \" + str(self.threshold)\n",
    "        str_type = margin + self.criterion_type + \" = \" + str(self.criterion_value)\n",
    "        label = margin + \"labels: \" + str(self.labels[0] if len(self.labels) != 0 else \"No label\")\n",
    "        left_node = margin + \"left: \\n\" + ('' if self.left != None else margin + ' ' * 4) + str(self.left)\n",
    "        right_node = margin + \"right: \\n\" + ('' if self.right != None else margin + ' ' * 4) + str(self.right)\n",
    "        return feature + '\\n' +\\\n",
    "               label + '\\n' +\\\n",
    "               str_threshhold + '\\n' +\\\n",
    "               str_type + '\\n' +\\\n",
    "               left_node + '\\n' +\\\n",
    "               right_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the function for calculating a prediction in a leaf. For regression, let's take the mean for all values in a leaf, for classification - the most popular class in leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Criterion(Enum):\n",
    "    ENTHROPY = 'enthropy'\n",
    "    GINI = 'gini'\n",
    "    VARIANCE = 'variance'\n",
    "    MAD_MEDIAN = 'mad_median'\n",
    "\n",
    "    def criterion_func(self):\n",
    "        if self == Criterion.GINI:\n",
    "            return gini\n",
    "        elif self == Criterion.ENTHROPY:\n",
    "            return entropy\n",
    "        elif self == Criterion.VARIANCE:\n",
    "            return variance\n",
    "        else:\n",
    "            return mad_median\n",
    "\n",
    "class DecisionTree(BaseEstimator):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_depth=np.inf,\n",
    "                 min_samples_split=2, \n",
    "                 criterion=Criterion.GINI,\n",
    "                 debug=False):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.debug = debug\n",
    "        self.primaryNode = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.primaryNode = self.__build(X_train=X, \n",
    "                                        Y_train=y,\n",
    "                                        criterion=self.criterion)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "#         for record in X:\n",
    "        pass\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    \n",
    "    def __build(self,\n",
    "                X_train=pd.DataFrame(),\n",
    "                Y_train=pd.DataFrame(),\n",
    "                current_criterion_value=None,\n",
    "                iteration=0,\n",
    "                criterion=Criterion.GINI):\n",
    "\n",
    "        Func = criterion.criterion_func()\n",
    "        # root node\n",
    "        t = Node(feature_idx=-1,\n",
    "                 threshold=0,\n",
    "                 left=None,\n",
    "                 right=None,\n",
    "                 criterion=criterion)\n",
    "        t.criterion_type = str(criterion.value)\n",
    "\n",
    "        best_params = None\n",
    "\n",
    "        if (current_criterion_value != None and \\\n",
    "            (current_criterion_value <= 0.01 or iteration >= self.max_depth)) or \\\n",
    "            (len(X_train) == 0 or len(Y_train) == 0) or \\\n",
    "            (len(X_train) + len(Y_train) < self.min_samples_split):\n",
    "            \n",
    "            return t\n",
    "        else:\n",
    "            iteration += 1\n",
    "#             print(\"iteration: \" + str(iteration))\n",
    "#             print(\"Y_train columns: \" + str(Y_train.columns[0]))\n",
    "            for column in X_train.columns:\n",
    "                current_feature = X_train[column]\n",
    "                frame = {column: current_feature, Y_train.columns[0]: Y_train[Y_train.columns[0]]}\n",
    "                combined_feature = pd.DataFrame(frame)\n",
    "                sorted_feature = combined_feature.sort_values(column)\n",
    "                previous_result = sorted_feature.iloc[0][Y_train.columns[0]]\n",
    "\n",
    "                current_result = None\n",
    "                idx_to_check = []\n",
    "\n",
    "                # index_list = list(sorted_feature.index.values)\n",
    "                # print(\"Indexes list:\")\n",
    "                # print(index_list)\n",
    "                # for index in list(sorted_feature.index.values):\n",
    "                #     # current_row = sorted_feature.loc[index]\n",
    "                #     # current_result = sorted_feature.iloc[index, 1]\n",
    "                #     current_result = sorted_feature.loc[index][str(Y_train.columns[0])]\n",
    "                #     if current_result != previous_result:\n",
    "                #         idx_to_check.append(index)\n",
    "                #     previous_result = current_result\n",
    "                #\n",
    "                # #             print(idx_to_check)\n",
    "                # if len(idx_to_check) == 0:\n",
    "                #     break\n",
    "                # calculate entropy gain or giny impyty reduction for current feature\n",
    "                part_result = best_partition_functional_result(list(sorted_feature[column]),\n",
    "                                                               list(sorted_feature[Y_train.columns[0]]),\n",
    "                                                               F=Func)\n",
    "                \n",
    "                mean_gain_paramter = (part_result[4] + part_result[5]) / 2\n",
    "                if best_params == None or (best_params[4] + best_params[5]) / 2 > mean_gain_paramter:\n",
    "                    best_params = part_result\n",
    "\n",
    "                    t.feature_idx = X_train.columns.get_loc(column)\n",
    "                    t.labels = [column]\n",
    "                    current_criterion_value = best_params[6]\n",
    "                    t.criterion_value = current_criterion_value\n",
    "\n",
    "            combined_data_set = pd.concat([X_train, Y_train], axis=1, sort=False)\n",
    "            combined_data_set.sort_values(t.labels[0], inplace=True)\n",
    "\n",
    "            t.threshold = best_params[7]\n",
    "            X_left = combined_data_set.iloc[0:best_params[8], np.arange(0, len(X_train.columns))]\n",
    "            X_right = combined_data_set.iloc[best_params[8]:len(combined_data_set),\n",
    "                      np.arange(0, len(X_train.columns))]\n",
    "            Y_left = pd.DataFrame(combined_data_set.iloc[0:best_params[8], len(X_train.columns)])\n",
    "            Y_right = pd.DataFrame(combined_data_set.iloc[best_params[8]:len(combined_data_set),\n",
    "                                   len(X_train.columns)])\n",
    "#             print(\"X_left\")\n",
    "#             print(X_left)\n",
    "#             print(\"X_right\")\n",
    "#             print(X_right)\n",
    "#             print(\"Y_left\")\n",
    "#             print(Y_left)\n",
    "#             print('Y_right')\n",
    "#             print(Y_right)\n",
    "            t.left = self.__build(X_left,\n",
    "                                  Y_left,\n",
    "                                  current_criterion_value=(best_params[4] + best_params[5]) / 2,\n",
    "                                  iteration=iteration,\n",
    "                                  criterion=criterion)\n",
    "            t.right = self.__build(X_right,\n",
    "                                   Y_right,\n",
    "                                   current_criterion_value=(best_params[4] + best_params[5]) / 2,\n",
    "                                   iteration=iteration,\n",
    "                                   criterion=criterion)\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([64, 80])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common = pd.DataFrame({'Возраст':  [17, 64, 18, 20, 38, 49, 55, 25, 29, 31, 33],\n",
    "                          'Зарплата': [25, 80, 22, 36, 37, 59, 74, 70, 33, 102, 88],\n",
    "                          'Невозврат кредита': [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]})\n",
    "\n",
    "tree = DecisionTree(max_depth=4, min_samples_split=2, criterion=Criterion.GINI)\n",
    "trained_tree = tree.fit(X=df_common.drop(columns=['Невозврат кредита'], axis=0),\n",
    "                        y=df_common.drop(columns=['Возраст', 'Зарплата'], axis=0))\n",
    "# print(trained_tree.primaryNode)\n",
    "\n",
    "# for record in df_common['Возраст', 'Зарплата']:\n",
    "#     print(record)\n",
    "\n",
    "# df_common.values[0]\n",
    "df_common[['Возраст', 'Зарплата']][df_common['Зарплата'] > 70].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the implemented algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `digits` using the method `load_digits`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, and `random_state=17`. Try to train shallow decision trees and make sure that gini and entropy criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use range(3, 11), for criterion use {'gini', 'entropy'}. Quality measure is `scoring`='accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `accuracy` for criteria `gini` and `entropy` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Choose all correct statements:**\n",
    "1. Optimal value of the `max_depth` parameter is on the interval [4, 9] for both criteria.\n",
    "2. Created plots have no intersection on the interval [3, 10]\n",
    "3. Created plots intersect each other only once on the interval [3, 10].\n",
    "4. The best quality for `max_depth` on the interval [3, 10] is reached using `gini` criterion .\n",
    "5. Accuracy is strictly increasing at least for one of the criteria, when `max_depth` is also increasing on the interval [3, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the optimal values for max_depth and criterion parameters?**\n",
    "1. max_depth = 7, criterion = 'gini';\n",
    "2. max_depth = 7, criterion = 'entropy';\n",
    "3. max_depth = 10, criterion = 'entropy';\n",
    "4. max_depth = 10, criterion = 'gini';\n",
    "5. max_depth = 9, criterion = 'entropy';\n",
    "6. max_depth = 9, criterion = 'gini';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train decision tree on `(X_train, y_train)` using the optimal values of `max_depth` and `criterion`. Compute class probabilities for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given matrix, compute the mean class probabilities for all instances in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is the maximum probability in a resulted vector?**\n",
    "1. 0.127\n",
    "2. 0.118\n",
    "3. 1.0\n",
    "4. 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset `boston` using the method `load_boston`. Split the data into train and test with the `train_test_split` method, use parameter values `test_size=0.2`, `random_state=17`. Try to train shallow regression decision trees and make sure that `variance` and `mad_median` criteria return different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5-folds cross-validation (`GridSearchCV`) pick up the optimal values of the `max_depth` and `criterion` parameters. For the parameter `max_depth` use `range(2, 9)`, for `criterion` use {'variance', 'mad_median'}. Quality measure is `scoring`='neg_mean_squared_error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the plot of the mean quality measure `neg_mean_squared_error` for criteria `variance` and `mad_median` depending on `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Choose all correct statements:**\n",
    "1. Created plots have no intersection on the interval [2, 8].\n",
    "2. Created plots intersect each other only once on the interval [2, 8].\n",
    "3. Optimal value of the `max_depth` for each of the criteria is on the border of the interval [2, 8].\n",
    "4. The best quality at `max_depth` on the interval [2, 8] is reached using `mad_median` criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What are the optimal values for `max_depth` and `criterion` parameters?**\n",
    "1. max_depth = 9, criterion = 'variance';\n",
    "2. max_depth = 5, criterion = 'mad_median';\n",
    "3. max_depth = 4, criterion = 'variance';\n",
    "4. max_depth = 2, criterion = 'mad_median';\n",
    "5. max_depth = 4, criterion = 'mad_median';\n",
    "6. max_depth = 5, criterion = 'variance'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "lesson4_part2_Decision_trees.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
